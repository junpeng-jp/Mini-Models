{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJXdSd61PwsQ",
        "colab_type": "text"
      },
      "source": [
        "# Overview of the AlexNet Unit\n",
        "\n",
        "The AlexNet can be broken down into a basic unit architecture. The AlexNext Unit can be described as follows:\n",
        "\n",
        "<div>\n",
        "<img src=\"./assets/AlexNetUnit.png\" width = 800px>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xuRDgUVPwsR",
        "colab_type": "text"
      },
      "source": [
        "## Local Response normalisation (LRNorm)\n",
        "AlexNet employs the idea of local response normalisation. The outputs from the of n convolution layers will be normalised using the following formula:\n",
        "\n",
        "$$ b^i_{x, y} = a^i_{x, y} / \\biggl( k + \\alpha\\sum_{j = max(0, i-n/2)}^{min(N-1,i+n/2)} (a^j_{x, y})^2 \\biggr)^{\\beta} $$\n",
        "\n",
        "Essentially, LRNorm normalises $n$ consecutive output of the $N$ convolutation kernels. One efficient way to implement this is to 3D average pool over the channels of the tensor as implmement in PyTorch.\n",
        "\n",
        "(Todo: add LRNorm animation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Avo0DZPwsR",
        "colab_type": "text"
      },
      "source": [
        "## Overlap Max Pooling\n",
        "\n",
        "Usually, traditional max pooling layers will have their stride the same length as the kernel size. \n",
        "\n",
        "In AlexNet, Max Pooling layers with stride = 2 & kernel size = 3 were used, resulting in some overlaps between each max pooling layer. The paper shared that this helps to avoid overfitting from their training experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_iXvRh8PwsT",
        "colab_type": "text"
      },
      "source": [
        "## Overcoming 2012 GPU Constraints\n",
        "\n",
        "AlexNet had to work around the GPU computation limitations back in 2012. To utilise some form of parallel computing during training, the number of kernels in each layer of the network was split equally among 2 GPUs. \n",
        "\n",
        "Unless specifically concatenated, the tensors from the 2 GPUs are passed on to the next layer in the same GPU and will not interact with the other GPUs.\n",
        "\n",
        "<div>\n",
        "<img src=\"./assets/AlexNet_Parallel.png\" width = 800px>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5ut2YusYPwsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KrKcM7CzPwsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the abstraction for a AlexNet Unit\n",
        "# It takes 2 inputs x1 and x2 as the training is parallelised over 2 GPUs\n",
        "# In the even that there is inter-GPU interaction before the layer, x1 & x2 will be the same tensor\n",
        "class AlexNetUnit(torch.nn.Module):\n",
        "    _NORM_N = 5\n",
        "    _NORM_k = 2\n",
        "    _NORM_a = 10**-4\n",
        "    _NORM_b = 0.75\n",
        "    _POOL_stride = 2\n",
        "    _POOL_kernel = 3\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, \n",
        "        stride, padding, hasLRNorm = False, hasOLPool = False):\n",
        "        super().__init__()\n",
        "        self.hasLRNorm = hasLRNorm\n",
        "        self.hasOLPool = hasOLPool\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        if self.hasLRNorm:\n",
        "            self.lrNorm = nn.LocalResponseNorm(self._NORM_N, self._NORM_a, self._NORM_b, self._NORM_k)\n",
        "        \n",
        "        if self.hasOLPool:\n",
        "            self.olPool = nn.MaxPool2d(self._POOL_kernel, self._POOL_stride)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        out = self.conv(input)\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        if self.hasLRNorm:\n",
        "            out = self.lrNorm(out)\n",
        "        if self.hasOLPool:\n",
        "            out = self.olPool(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9bZn-eNPwsY",
        "colab_type": "text"
      },
      "source": [
        "## The Original AlexNet Structure\n",
        "\n",
        "There are 5 AlexNet Units, 2 Fully Connected layers, and 1 Output layer in the AlexNet. The network splits the channels in half, effectively conducting parallel training over 2 GPUs. A diagram of the condensed output is shown below:\n",
        "\n",
        "<div>\n",
        "<img src=\"./assets/AlexNet-2012.png\" width = 800px>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZv9mBLHPwsZ",
        "colab_type": "text"
      },
      "source": [
        "## A Smaller AlexNet for CIFAR-10\n",
        "\n",
        "As a demonstration, we will train a Simplified AlexNet with have the network size. We will also not be training on the 256x256 RGB images in ImageNet. Instead, we will be training on the smaller 32x32 CIFAR-10 RGB dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7uZSVzUjPwsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlexNet(torch.nn.Module):\n",
        "    def __init__(self, nClass):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            AlexNetUnit(3, 48, (7,7), 1, 2, True, True),\n",
        "            AlexNetUnit(48, 128, (5,5), 1, 2, True, True),\n",
        "            AlexNetUnit(128, 192, (3,3), 1, 1),\n",
        "            AlexNetUnit(192, 128, (3,3), 1, 1, hasOLPool=True)\n",
        "        ) \n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128*2*2, 2048),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Linear(2048, nClass)\n",
        "        )\n",
        "        \n",
        "    def forward(self, input):\n",
        "        out = self.features(input)\n",
        "        out = out.reshape(-1, 128*2*2)\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW2vw84tPwsb",
        "colab_type": "text"
      },
      "source": [
        "## CIFAR-10 Dataset\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "Source: [https://cs.toronto.edu/!kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "\n",
        "![CIFAR-10 Dataset Sample](./assets/CIFAR10.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4Wp_g_3NPwsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn.init import kaiming_normal_, normal_\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YaHDqAUcPwsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initWeight(unit):\n",
        "    if isinstance(unit, (torch.nn.Linear, torch.nn.Conv2d)):\n",
        "        kaiming_normal_(unit.weight, nonlinearity='relu')\n",
        "        normal_(unit.bias)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NKw7ZpUiPwsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "hyperparam = {\n",
        "    'nEpoch': 30,\n",
        "    'batchSize': 256\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "apDbEs5nPwsi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "ac6a0752fcf7416f90206c82b9f6bf1e",
            "f0e077a34a2644b8bdfc2b4affd8ad9d",
            "b2af6c2824f34bcaafbb87fb3f936838",
            "fe616813720749228f0f0f0514c2da2b",
            "29f3a443d1bc4dc186cc4b594fd4a320",
            "8be3d74054c24558b626e79d7e3692ac",
            "b1c476e8eb5b405cb90e815d0cc8d2be",
            "b5568fdff9234774a609844a14e4767c"
          ]
        },
        "outputId": "4b2c896c-392a-475e-82c8-4e40b6657869"
      },
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "trainset = CIFAR10(\n",
        "    root = \"./data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "testset = CIFAR10(\n",
        "    root = \"./data\",\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac6a0752fcf7416f90206c82b9f6bf1e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8UwyMJG3Pwsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader = DataLoader(trainset, batch_size=hyperparam['batchSize'], shuffle=True, num_workers=0)\n",
        "testloader = DataLoader(testset, batch_size=hyperparam['batchSize'], shuffle=True, num_workers=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DjUztNmoPwsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = AlexNet(10)\n",
        "model.apply(initWeight)\n",
        "model.to(device)\n",
        "\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "#sticking to Adam's default hyperparameters from the original paper\n",
        "optim = Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6KRciU0_Pwsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e0ff2a9-3bbe-4047-fa8d-e5f6cb1c9742"
      },
      "source": [
        "for epoch in range(hyperparam['nEpoch']):\n",
        "    for i, (x, y) in enumerate(trainloader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(x)\n",
        "        loss = lossFn(outputs, y)\n",
        "        \n",
        "        # Backward pass\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Parameter update\n",
        "        optim.step()\n",
        "\n",
        "        # Console log Progress\n",
        "        if (i+1) % 50 == 0 or i + 1 == len(trainloader):\n",
        "            print(f'Epoch [{epoch + 1}/{hyperparam[\"nEpoch\"]}], Step [{i+1}/{len(trainloader)}], Loss: {loss:.4f}')\n",
        "                   "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Step [50/196], Loss: 2.3322\n",
            "Epoch [1/30], Step [100/196], Loss: 2.2207\n",
            "Epoch [1/30], Step [150/196], Loss: 2.0320\n",
            "Epoch [1/30], Step [196/196], Loss: 1.9698\n",
            "Epoch [2/30], Step [50/196], Loss: 1.9745\n",
            "Epoch [2/30], Step [100/196], Loss: 1.9818\n",
            "Epoch [2/30], Step [150/196], Loss: 1.7343\n",
            "Epoch [2/30], Step [196/196], Loss: 1.8631\n",
            "Epoch [3/30], Step [50/196], Loss: 1.8131\n",
            "Epoch [3/30], Step [100/196], Loss: 1.6246\n",
            "Epoch [3/30], Step [150/196], Loss: 1.5707\n",
            "Epoch [3/30], Step [196/196], Loss: 1.6057\n",
            "Epoch [4/30], Step [50/196], Loss: 1.4629\n",
            "Epoch [4/30], Step [100/196], Loss: 1.6090\n",
            "Epoch [4/30], Step [150/196], Loss: 1.6101\n",
            "Epoch [4/30], Step [196/196], Loss: 1.5129\n",
            "Epoch [5/30], Step [50/196], Loss: 1.4618\n",
            "Epoch [5/30], Step [100/196], Loss: 1.5029\n",
            "Epoch [5/30], Step [150/196], Loss: 1.5305\n",
            "Epoch [5/30], Step [196/196], Loss: 1.4581\n",
            "Epoch [6/30], Step [50/196], Loss: 1.3011\n",
            "Epoch [6/30], Step [100/196], Loss: 1.4397\n",
            "Epoch [6/30], Step [150/196], Loss: 1.3049\n",
            "Epoch [6/30], Step [196/196], Loss: 1.1761\n",
            "Epoch [7/30], Step [50/196], Loss: 1.2618\n",
            "Epoch [7/30], Step [100/196], Loss: 1.3645\n",
            "Epoch [7/30], Step [150/196], Loss: 1.2114\n",
            "Epoch [7/30], Step [196/196], Loss: 1.0458\n",
            "Epoch [8/30], Step [50/196], Loss: 1.2653\n",
            "Epoch [8/30], Step [100/196], Loss: 1.2085\n",
            "Epoch [8/30], Step [150/196], Loss: 1.1779\n",
            "Epoch [8/30], Step [196/196], Loss: 1.0861\n",
            "Epoch [9/30], Step [50/196], Loss: 1.2049\n",
            "Epoch [9/30], Step [100/196], Loss: 1.1979\n",
            "Epoch [9/30], Step [150/196], Loss: 1.1099\n",
            "Epoch [9/30], Step [196/196], Loss: 1.2781\n",
            "Epoch [10/30], Step [50/196], Loss: 1.2145\n",
            "Epoch [10/30], Step [100/196], Loss: 1.1010\n",
            "Epoch [10/30], Step [150/196], Loss: 1.0641\n",
            "Epoch [10/30], Step [196/196], Loss: 1.1624\n",
            "Epoch [11/30], Step [50/196], Loss: 1.0928\n",
            "Epoch [11/30], Step [100/196], Loss: 1.1821\n",
            "Epoch [11/30], Step [150/196], Loss: 1.1180\n",
            "Epoch [11/30], Step [196/196], Loss: 1.1216\n",
            "Epoch [12/30], Step [50/196], Loss: 0.9685\n",
            "Epoch [12/30], Step [100/196], Loss: 1.0235\n",
            "Epoch [12/30], Step [150/196], Loss: 0.9949\n",
            "Epoch [12/30], Step [196/196], Loss: 0.8880\n",
            "Epoch [13/30], Step [50/196], Loss: 0.9503\n",
            "Epoch [13/30], Step [100/196], Loss: 0.9877\n",
            "Epoch [13/30], Step [150/196], Loss: 0.8556\n",
            "Epoch [13/30], Step [196/196], Loss: 1.3245\n",
            "Epoch [14/30], Step [50/196], Loss: 0.9969\n",
            "Epoch [14/30], Step [100/196], Loss: 0.9408\n",
            "Epoch [14/30], Step [150/196], Loss: 1.0194\n",
            "Epoch [14/30], Step [196/196], Loss: 1.1736\n",
            "Epoch [15/30], Step [50/196], Loss: 0.8742\n",
            "Epoch [15/30], Step [100/196], Loss: 0.8227\n",
            "Epoch [15/30], Step [150/196], Loss: 0.7802\n",
            "Epoch [15/30], Step [196/196], Loss: 0.9709\n",
            "Epoch [16/30], Step [50/196], Loss: 0.8058\n",
            "Epoch [16/30], Step [100/196], Loss: 0.8388\n",
            "Epoch [16/30], Step [150/196], Loss: 0.8782\n",
            "Epoch [16/30], Step [196/196], Loss: 0.9304\n",
            "Epoch [17/30], Step [50/196], Loss: 0.8275\n",
            "Epoch [17/30], Step [100/196], Loss: 0.8440\n",
            "Epoch [17/30], Step [150/196], Loss: 0.8437\n",
            "Epoch [17/30], Step [196/196], Loss: 0.9241\n",
            "Epoch [18/30], Step [50/196], Loss: 0.7676\n",
            "Epoch [18/30], Step [100/196], Loss: 0.6919\n",
            "Epoch [18/30], Step [150/196], Loss: 0.7292\n",
            "Epoch [18/30], Step [196/196], Loss: 0.9951\n",
            "Epoch [19/30], Step [50/196], Loss: 0.6173\n",
            "Epoch [19/30], Step [100/196], Loss: 0.7104\n",
            "Epoch [19/30], Step [150/196], Loss: 0.8452\n",
            "Epoch [19/30], Step [196/196], Loss: 0.9125\n",
            "Epoch [20/30], Step [50/196], Loss: 0.6925\n",
            "Epoch [20/30], Step [100/196], Loss: 0.6885\n",
            "Epoch [20/30], Step [150/196], Loss: 0.6013\n",
            "Epoch [20/30], Step [196/196], Loss: 0.6522\n",
            "Epoch [21/30], Step [50/196], Loss: 0.7216\n",
            "Epoch [21/30], Step [100/196], Loss: 0.6663\n",
            "Epoch [21/30], Step [150/196], Loss: 0.7811\n",
            "Epoch [21/30], Step [196/196], Loss: 0.7332\n",
            "Epoch [22/30], Step [50/196], Loss: 0.7111\n",
            "Epoch [22/30], Step [100/196], Loss: 0.7049\n",
            "Epoch [22/30], Step [150/196], Loss: 0.6961\n",
            "Epoch [22/30], Step [196/196], Loss: 0.6531\n",
            "Epoch [23/30], Step [50/196], Loss: 0.6760\n",
            "Epoch [23/30], Step [100/196], Loss: 0.6534\n",
            "Epoch [23/30], Step [150/196], Loss: 0.6302\n",
            "Epoch [23/30], Step [196/196], Loss: 0.8024\n",
            "Epoch [24/30], Step [50/196], Loss: 0.7141\n",
            "Epoch [24/30], Step [100/196], Loss: 0.7576\n",
            "Epoch [24/30], Step [150/196], Loss: 0.6440\n",
            "Epoch [24/30], Step [196/196], Loss: 0.7584\n",
            "Epoch [25/30], Step [50/196], Loss: 0.7358\n",
            "Epoch [25/30], Step [100/196], Loss: 0.6952\n",
            "Epoch [25/30], Step [150/196], Loss: 0.6443\n",
            "Epoch [25/30], Step [196/196], Loss: 0.4791\n",
            "Epoch [26/30], Step [50/196], Loss: 0.6143\n",
            "Epoch [26/30], Step [100/196], Loss: 0.5624\n",
            "Epoch [26/30], Step [150/196], Loss: 0.6222\n",
            "Epoch [26/30], Step [196/196], Loss: 0.5796\n",
            "Epoch [27/30], Step [50/196], Loss: 0.5526\n",
            "Epoch [27/30], Step [100/196], Loss: 0.5924\n",
            "Epoch [27/30], Step [150/196], Loss: 0.4990\n",
            "Epoch [27/30], Step [196/196], Loss: 0.6781\n",
            "Epoch [28/30], Step [50/196], Loss: 0.5809\n",
            "Epoch [28/30], Step [100/196], Loss: 0.6034\n",
            "Epoch [28/30], Step [150/196], Loss: 0.5663\n",
            "Epoch [28/30], Step [196/196], Loss: 0.6161\n",
            "Epoch [29/30], Step [50/196], Loss: 0.5048\n",
            "Epoch [29/30], Step [100/196], Loss: 0.5539\n",
            "Epoch [29/30], Step [150/196], Loss: 0.6645\n",
            "Epoch [29/30], Step [196/196], Loss: 0.4022\n",
            "Epoch [30/30], Step [50/196], Loss: 0.4863\n",
            "Epoch [30/30], Step [100/196], Loss: 0.5948\n",
            "Epoch [30/30], Step [150/196], Loss: 0.6080\n",
            "Epoch [30/30], Step [196/196], Loss: 0.4243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fIGckPi6Pwsr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "142495f8-9cac-450c-e709-32cfac0076aa"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x, y in testloader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        prob = model(x)\n",
        "        predicted = prob.argmax(axis=1)\n",
        "        total += x.size(0)\n",
        "        correct += (predicted == y).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 70.82 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "AlexNet in PyTorch.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac6a0752fcf7416f90206c82b9f6bf1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f0e077a34a2644b8bdfc2b4affd8ad9d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b2af6c2824f34bcaafbb87fb3f936838",
              "IPY_MODEL_fe616813720749228f0f0f0514c2da2b"
            ]
          }
        },
        "f0e077a34a2644b8bdfc2b4affd8ad9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2af6c2824f34bcaafbb87fb3f936838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_29f3a443d1bc4dc186cc4b594fd4a320",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8be3d74054c24558b626e79d7e3692ac"
          }
        },
        "fe616813720749228f0f0f0514c2da2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b1c476e8eb5b405cb90e815d0cc8d2be",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 33758116.52it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5568fdff9234774a609844a14e4767c"
          }
        },
        "29f3a443d1bc4dc186cc4b594fd4a320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8be3d74054c24558b626e79d7e3692ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1c476e8eb5b405cb90e815d0cc8d2be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b5568fdff9234774a609844a14e4767c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}