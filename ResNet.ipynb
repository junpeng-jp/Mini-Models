{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "Papers:\n",
    "\n",
    "[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "[Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/pdf/1611.05431.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degradation of Training Accuracy in Deep Neural Networks\n",
    "\n",
    "Researchers have identified an issue where adding more layers to deep networks result in poorer training errors. \n",
    "\n",
    "The authors of the first paper isolated this issue from vanishing gradients through an experiment involving stacking identity layers. This issue is termed as degradation and the paper aims to address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Residuals\n",
    "\n",
    "The degradation problem highlighted difficulties in fitting optimal complex functions in Deep Neural Networks.\n",
    "\n",
    "To tackle this, the authors reformulated the optimal function into 2 parts - the original inputs $x$ and the residuals $F(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Types of Residual Connections\n",
    "\n",
    "There are 2 types of residual unit proposed in the paper:\n",
    "\n",
    "<div>\n",
    "<img src=\"./assets/ResNetUnit.png\" width = 800px>\n",
    "</div>\n",
    "\n",
    "| Non-bottleneck | Bottleneck |\n",
    "|----------------|------------|\n",
    "| Effectively solves the degradation issue | Degradation still observed |\n",
    "| Requires more computation resource | More economical |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(inChannel, outChannel, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inChannel, outChannel, 1, stride=stride),\n",
    "        nn.BatchNorm2d(outChannel)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBasic(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def unit(inChannel, outChannel, filterSize=3, stride=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(inChannel, outChannel, filterSize, stride=stride, padding=filterSize//2),\n",
    "            nn.BatchNorm2d(outChannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outChannel, outChannel, filterSize, padding=filterSize//2),\n",
    "            nn.BatchNorm2d(outChannel)\n",
    "        )\n",
    "\n",
    "    def __init__(self, inChannel, outChannel, filterSize=3, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = ResBasic.unit(inChannel, outChannel, filterSize, stride)\n",
    "        self.projection = projection(inChannel, outChannel, stride) if stride > 1 else None\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.residual(input)\n",
    "        eye = input\n",
    "        if self.projection:\n",
    "            eye = self.projection(eye)\n",
    "        \n",
    "        out += eye\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBottleneck(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def unit(inChannel, embedDim, outChannel, filterSize=3, stride=1, groups=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(inChannel, embedDim, 1),\n",
    "            nn.BatchNorm2d(embedDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(embedDim, embedDim, filterSize, stride=stride, padding=filterSize//2, groups = groups),\n",
    "            nn.BatchNorm2d(embedDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(embedDim, outChannel, 1),\n",
    "            nn.BatchNorm2d(outChannel)\n",
    "        )\n",
    "\n",
    "    def __init__(self, inChannel, embedDim, outChannel, filterSize=3, stride=1, groups=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.residual = ResBottleneck.unit(inChannel, embedDim, outChannel, filterSize, stride, groups)\n",
    "        self.projection = projection(inChannel, outChannel, stride) if stride > 1 else None\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.residual(input)\n",
    "        eye = input\n",
    "        if self.projection:\n",
    "            eye = self.projection(eye)\n",
    "        \n",
    "        out += eye\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet20(torch.nn.Module):\n",
    "    def __init__(self, nClass):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            ResBasic(16, 16, 3, stride = 1),\n",
    "            ResBasic(16, 16, 3, stride = 1),\n",
    "            ResBasic(16, 16, 3, stride = 1),\n",
    "            ResBasic(16, 16, 3, stride = 1),\n",
    "            ResBasic(16, 16, 3, stride = 1),\n",
    "            ResBasic(16, 16, 3, stride = 1)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            ResBasic(16, 32, 3, stride = 2),\n",
    "            ResBasic(32, 32, 3, stride = 1),\n",
    "            ResBasic(32, 32, 3, stride = 1),\n",
    "            ResBasic(32, 32, 3, stride = 1),\n",
    "            ResBasic(32, 32, 3, stride = 1),\n",
    "            ResBasic(32, 32, 3, stride = 1)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            ResBasic(32, 64, 3, stride = 2),\n",
    "            ResBasic(64, 64, 3, stride = 1),\n",
    "            ResBasic(64, 64, 3, stride = 1),\n",
    "            ResBasic(64, 64, 3, stride = 1),\n",
    "            ResBasic(64, 64, 3, stride = 1),\n",
    "            ResBasic(64, 64, 3, stride = 1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AvgPool2d(8),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, nClass)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out = self.conv1(input)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resNetInit(l):\n",
    "    if isinstance(l, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(l.weight, mode='fan_out', nonlinearity='relu')\n",
    "    elif isinstance(l, nn.BatchNorm2d):\n",
    "        nn.init.constant_(l.weight, 1)\n",
    "        nn.init.constant_(l.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn.init import kaiming_normal_, normal_\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from helper import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# seeding the random number generators\n",
    "# ensures some form of determinism in the outputs \n",
    "seed = 2020\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "os.environ['PYTHONHASHSEED']=str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Files already downloaded and verified\nFiles already downloaded and verified\n"
    }
   ],
   "source": [
    "# We will be using the CIFAR-10 dataset\n",
    "trainset = CIFAR10(\n",
    "    root = \"../data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "testset = CIFAR10(\n",
    "    root = \"../data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = training.Trainer(nEpoch=1, logInterval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.addDataloader(\n",
    "    dataloader = DataLoader(\n",
    "        trainset, batch_size=256,\n",
    "        shuffle=True, num_workers=0),\n",
    "    loaderType = 'train')\n",
    "\n",
    "trainer.addDataloader(\n",
    "    dataloader = DataLoader(\n",
    "        testset, batch_size=256, \n",
    "        shuffle=True, num_workers=0),\n",
    "    loaderType = 'test')\n",
    "\n",
    "trainer.addLossFn(nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet20(10)\n",
    "model.apply(resNetInit)\n",
    "\n",
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch [ 1 / 1 ]  Batch [ 50  / 196 ]  Loss: 1.6917\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-be344cbbbe4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Projects\\Mini Models\\helper\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model, optimizer)\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[1;31m# Parameter update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programming\\Miniconda\\envs\\alyx\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programming\\Miniconda\\envs\\alyx\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitalyxconda5e12d191f71047d98c62eaab66bc9bd3",
   "display_name": "Python 3.7.4 64-bit ('alyx': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}